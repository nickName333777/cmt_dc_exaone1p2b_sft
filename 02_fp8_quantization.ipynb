{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbcf1798",
   "metadata": {},
   "source": [
    "# 02. FP8 ì–‘ìí™” â€” EXAONE-4.0-1.2B\n",
    "\n",
    "**ì „ëµ:** í‰ê°€ ì„œë²„ê°€ L4 GPUì¸ë°, L4ëŠ” FP8 ì—°ì‚°ì„ í•˜ë“œì›¨ì–´ ë ˆë²¨ë¡œ ê°€ì†í•¨.  \n",
    "GPTQ W4A16ë³´ë‹¤ **ì¶”ë¡  ì†ë„ê°€ ë” ë¹ ë¥¼ ìˆ˜ ìˆëŠ”** í•µì‹¬ ì°¨ë³„í™” ê¸°ë²•.\n",
    "\n",
    "| ë°©ì‹ | ê°€ì¤‘ì¹˜ | í™œì„±í™” | L4 í•˜ë“œì›¨ì–´ ê°€ì† | ì˜ˆìƒ íš¨ê³¼ |\n",
    "|------|--------|--------|-----------------|----------|\n",
    "| GPTQ W4A16 (ë² ì´ìŠ¤ë¼ì¸) | 4bit | 16bit | âŒ | ì†ë„ ë³´í†µ |\n",
    "| **FP8 W8A8** | 8bit | 8bit | âœ… INT8/FP8 í…ì„œì½”ì–´ | **ì†ë„ ìµœê°•** |\n",
    "\n",
    "**í™˜ê²½:** Google Colab Free (T4 16GB)  \n",
    "**vLLM:** 0.14.1 (FP8 compressed-tensors í¬ë§· ì§€ì›)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b245479d",
   "metadata": {},
   "source": [
    "## 0. í™˜ê²½ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49949adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# FP8 ì§€ì› ì—¬ë¶€ í™•ì¸ (Ampere ì´ìƒ: A100, L4, H100 ë“±)\n",
    "# T4ëŠ” FP8 ë¯¸ì§€ì›ì´ì§€ë§Œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜/ì €ì¥ì€ ê°€ëŠ¥\n",
    "# â†’ í‰ê°€ ì„œë²„(L4)ì—ì„œ ì‹¤ì œ FP8 í•˜ë“œì›¨ì–´ ê°€ì† ì‹¤í–‰ë¨\n",
    "compute_cap = torch.cuda.get_device_capability()\n",
    "print(f\"Compute Capability: {compute_cap}\")\n",
    "if compute_cap[0] >= 8:\n",
    "    print(\"âœ… FP8 í•˜ë“œì›¨ì–´ ì—°ì‚° ì§€ì› (ì´ GPUì—ì„œ FP8 ì¶”ë¡  ê°€ëŠ¥)\")\n",
    "else:\n",
    "    print(\"âš ï¸  ì´ GPU(T4)ëŠ” FP8 í•˜ë“œì›¨ì–´ ì—°ì‚° ë¯¸ì§€ì›\")\n",
    "    print(\"   â†’ ì–‘ìí™”/ì €ì¥ì€ ì—¬ê¸°ì„œ, ì‹¤ì œ FP8 ê°€ì†ì€ í‰ê°€ ì„œë²„(L4)ì—ì„œ ë™ì‘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b67ce",
   "metadata": {},
   "source": [
    "## 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "\n",
    "> âš ï¸ ì„¤ì¹˜ í›„ **ëŸ°íƒ€ì„ ì¬ì‹œì‘** í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "    llmcompressor \\\n",
    "    \"compressed-tensors==0.13.0\" \\\n",
    "    \"transformers>=4.54.0\" \\\n",
    "    datasets \\\n",
    "    accelerate \\\n",
    "    sentencepiece\n",
    "\n",
    "print(\"\\nâœ… ì„¤ì¹˜ ì™„ë£Œ! ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082939b8",
   "metadata": {},
   "source": [
    "## 2. ì„¤ì •ê°’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FP8 ì‹¤í—˜ ì„¤ì •\n",
    "# ============================================================\n",
    "\n",
    "MODEL_ID = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
    "OUT_DIR  = \"./model_fp8\"\n",
    "\n",
    "DATASET_ID    = \"LGAI-EXAONE/MANTA-1M\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "NUM_CALIBRATION_SAMPLES = 512   # FP8ì€ ìƒëŒ€ì ìœ¼ë¡œ ìƒ˜í”Œ ìˆ˜ì— ëœ ë¯¼ê°\n",
    "MAX_SEQUENCE_LENGTH     = 1024\n",
    "\n",
    "# FP8 W8A8: ê°€ì¤‘ì¹˜ FP8 + í™œì„±í™” FP8 â†’ L4ì—ì„œ í•˜ë“œì›¨ì–´ ê°€ì†\n",
    "SCHEME  = \"FP8\"\n",
    "TARGETS = [\"Linear\"]\n",
    "IGNORE  = [\"embed_tokens\", \"lm_head\"]  # ì„ë² ë”© ë ˆì´ì–´ëŠ” FP8 ì œì™¸\n",
    "\n",
    "ZIP_NAME = \"submit_fp8\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"ëª¨ë¸        : {MODEL_ID}\")\n",
    "print(f\"ì–‘ìí™” ë°©ì‹ : {SCHEME} (W8A8, L4 í•˜ë“œì›¨ì–´ ê°€ì†)\")\n",
    "print(f\"ìº˜ë¦¬ë¸Œ ìƒ˜í”Œ : {NUM_CALIBRATION_SAMPLES}\")\n",
    "print(f\"ì‹œí€€ìŠ¤ ê¸¸ì´ : {MAX_SEQUENCE_LENGTH}\")\n",
    "print(f\"ì €ì¥ ê²½ë¡œ   : {OUT_DIR}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d540156c",
   "metadata": {},
   "source": [
    "## 3. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "print(\"ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"âœ… ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f615b",
   "metadata": {},
   "source": [
    "## 4. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feba7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(f\"ë°ì´í„° ë¡œë“œ: {NUM_CALIBRATION_SAMPLES}ìƒ˜í”Œ\")\n",
    "ds = load_dataset(DATASET_ID, split=f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\")\n",
    "\n",
    "def preprocess(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"conversations\"],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    }\n",
    "\n",
    "ds = ds.map(preprocess)\n",
    "print(f\"âœ… ë°ì´í„° ì¤€ë¹„: {len(ds)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a4c4bc",
   "metadata": {},
   "source": [
    "## 5. FP8 ì–‘ìí™” ì‹¤í–‰\n",
    "\n",
    "> T4ì—ì„œ ì•½ 10~20ë¶„ ì†Œìš” (GPTQë³´ë‹¤ ë¹ ë¦„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f06b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import QuantizationModifier\n",
    "import time\n",
    "\n",
    "print(f\"FP8 ì–‘ìí™” ì‹œì‘ (scheme={SCHEME})...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# FP8ì€ GPTQModifier ëŒ€ì‹  QuantizationModifier ì‚¬ìš©\n",
    "recipe = [\n",
    "    QuantizationModifier(\n",
    "        targets=TARGETS,\n",
    "        scheme=SCHEME,\n",
    "        ignore=IGNORE,\n",
    "    )\n",
    "]\n",
    "\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nâœ… FP8 ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce07864",
   "metadata": {},
   "source": [
    "## 6. ì €ì¥ & ì œì¶œ íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d948a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print(f\"ëª¨ë¸ ì €ì¥ ì¤‘: {OUT_DIR}\")\n",
    "\n",
    "model.save_pretrained(OUT_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "# íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "total_size = sum(f.stat().st_size for f in Path(OUT_DIR).iterdir()) / 1024**3\n",
    "print(f\"\\nì´ í¬ê¸°: {total_size:.2f} GB\")\n",
    "print(\"(FP8ì€ BF16 ëŒ€ë¹„ ì•½ 50% í¬ê¸°, GPTQ W4A16ë³´ë‹¤ëŠ” í¼)\")\n",
    "\n",
    "# config.jsonì˜ quantization_config í™•ì¸ (vLLMì´ ì´ê±¸ ì½ì–´ì„œ FP8 ì»¤ë„ ì„ íƒ)\n",
    "import json\n",
    "with open(f\"{OUT_DIR}/config.json\") as f:\n",
    "    cfg = json.load(f)\n",
    "print(f\"\\nquantization_config: {cfg.get('quantization_config', 'None')}\")\n",
    "\n",
    "print(\"\\nâœ… ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7992ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œìš© zip\n",
    "shutil.make_archive(base_name=ZIP_NAME, format=\"zip\", root_dir=\".\", base_dir=OUT_DIR)\n",
    "zip_size = Path(f\"{ZIP_NAME}.zip\").stat().st_size / 1024**3\n",
    "print(f\"âœ… {ZIP_NAME}.zip ìƒì„± ì™„ë£Œ ({zip_size:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996dea58",
   "metadata": {},
   "source": [
    "## 7. Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b6d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"ì €ì¥ëœ FP8 ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸...\")\n",
    "model_check = AutoModelForCausalLM.from_pretrained(\n",
    "    OUT_DIR,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tok_check = AutoTokenizer.from_pretrained(OUT_DIR, trust_remote_code=True)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”\"}]\n",
    "inputs = tok_check.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model_check.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_check.generate(inputs, max_new_tokens=50, do_sample=False)\n",
    "\n",
    "response = tok_check.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
    "print(f\"ëª¨ë¸ ì‘ë‹µ: {response}\")\n",
    "print(\"\\nâœ… FP8 ëª¨ë¸ ì •ìƒ ì‘ë™!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d1914",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ê¸°ë¡\n",
    "\n",
    "| ì‹¤í—˜ | SCHEME | ìƒ˜í”Œ ìˆ˜ | MaxLen | LB Score | ë¹„ê³  |\n",
    "|------|--------|---------|--------|----------|------|\n",
    "| FP8 ê¸°ë³¸ | FP8(W8A8) | 512 | 1024 | - | L4 í•˜ë“œì›¨ì–´ ê°€ì† ê¸°ëŒ€ |\n",
    "\n",
    "---\n",
    "## â­ï¸ ë‹¤ìŒ ë‹¨ê³„\n",
    "- FP8ì´ GPTQë³´ë‹¤ LB ì ìˆ˜ê°€ ë†’ìœ¼ë©´ â†’ FP8ì„ ë©”ì¸ ì „ëµìœ¼ë¡œ í™•ì •\n",
    "- ì„±ëŠ¥(ì •í™•ë„)ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ â†’ `04_qlora_sft.ipynb`ë¡œ QLoRA íšŒë³µ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
