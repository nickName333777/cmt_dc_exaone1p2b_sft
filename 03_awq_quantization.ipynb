{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c463fbac",
   "metadata": {},
   "source": [
    "# 03. AWQ ì–‘ìí™” â€” EXAONE-4.0-1.2B\n",
    "\n",
    "**ì „ëµ:** AWQ(Activation-aware Weight Quantization)ëŠ” GPTQì™€ ê°™ì€ 4-bitì´ì§€ë§Œ  \n",
    "í™œì„±í™” ë¶„í¬ë¥¼ ê³ ë ¤í•´ì„œ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ë¥¼ ë³´í˜¸í•˜ë¯€ë¡œ **ì •í™•ë„ ì†ì‹¤ì´ ë” ì ìŒ**.  \n",
    "vLLM 0.14.1ì—ì„œ AWQ ì»¤ë„ì„ ìµœì í™”í•´ë‘ì–´ì„œ ì¶”ë¡  ì†ë„ë„ ë¹ ë¥¸ í¸.\n",
    "\n",
    "| ë°©ì‹ | ì •í™•ë„ ìœ ì§€ | ì¶”ë¡  ì†ë„ | ëª¨ë¸ í¬ê¸° |\n",
    "|------|-----------|----------|----------|\n",
    "| GPTQ W4A16 | ë³´í†µ | ë³´í†µ | ì‘ìŒ |\n",
    "| **AWQ W4A16** | **ì¢‹ìŒ** | **ì¢‹ìŒ** | ì‘ìŒ |\n",
    "| FP8 W8A8 | ë§¤ìš° ì¢‹ìŒ | ìµœê³ (L4) | ë³´í†µ |\n",
    "\n",
    "**í™˜ê²½:** Google Colab Free (T4 16GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf57a7",
   "metadata": {},
   "source": [
    "## 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "\n",
    "> âš ï¸ ì„¤ì¹˜ í›„ **ëŸ°íƒ€ì„ ì¬ì‹œì‘** í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69281db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoawq: AWQ ì–‘ìí™” ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "!pip install -q \\\n",
    "    autoawq \\\n",
    "    \"transformers>=4.54.0\" \\\n",
    "    datasets \\\n",
    "    accelerate \\\n",
    "    sentencepiece\n",
    "\n",
    "print(\"\\nâœ… ì„¤ì¹˜ ì™„ë£Œ! ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876af151",
   "metadata": {},
   "source": [
    "## 2. ì„¤ì •ê°’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d235f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AWQ ì‹¤í—˜ ì„¤ì •\n",
    "# ============================================================\n",
    "\n",
    "MODEL_ID = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
    "OUT_DIR  = \"./model_awq\"\n",
    "\n",
    "# AWQ ì–‘ìí™” ì„¤ì •\n",
    "AWQ_CONFIG = {\n",
    "    \"zero_point\": True,          # Zero-point ì–‘ìí™” ì‚¬ìš© (ì •í™•ë„ í–¥ìƒ)\n",
    "    \"q_group_size\": 128,         # ê·¸ë£¹ í¬ê¸° (128ì´ í‘œì¤€)\n",
    "    \"w_bit\": 4,                  # 4-bit ê°€ì¤‘ì¹˜\n",
    "    \"version\": \"GEMM\",           # vLLM í˜¸í™˜ ì»¤ë„ ë²„ì „\n",
    "}\n",
    "\n",
    "DATASET_ID    = \"LGAI-EXAONE/MANTA-1M\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "NUM_CALIBRATION_SAMPLES = 512\n",
    "MAX_SEQUENCE_LENGTH     = 1024\n",
    "\n",
    "ZIP_NAME = \"submit_awq\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"ëª¨ë¸        : {MODEL_ID}\")\n",
    "print(f\"AWQ ì„¤ì •    : {AWQ_CONFIG}\")\n",
    "print(f\"ìº˜ë¦¬ë¸Œ ìƒ˜í”Œ : {NUM_CALIBRATION_SAMPLES}\")\n",
    "print(f\"ì‹œí€€ìŠ¤ ê¸¸ì´ : {MAX_SEQUENCE_LENGTH}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca919de",
   "metadata": {},
   "source": [
    "## 3. ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d937b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "print(\"ëª¨ë¸ ë¡œë“œ ì¤‘... (AWQ ì „ìš© ë¡œë” ì‚¬ìš©)\")\n",
    "model = AutoAWQForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    safetensors=True,\n",
    ")\n",
    "print(\"âœ… ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ebc01",
   "metadata": {},
   "source": [
    "## 4. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b169435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(f\"ë°ì´í„° ë¡œë“œ: {NUM_CALIBRATION_SAMPLES}ìƒ˜í”Œ\")\n",
    "ds = load_dataset(DATASET_ID, split=f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\")\n",
    "\n",
    "# AWQëŠ” í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°”ë¡œ ë°›ìŒ\n",
    "def preprocess(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"conversations\"],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    }\n",
    "\n",
    "ds = ds.map(preprocess)\n",
    "calib_data = ds[\"text\"]  # AWQëŠ” í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬\n",
    "print(f\"âœ… ë°ì´í„° ì¤€ë¹„: {len(calib_data)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1f4b1",
   "metadata": {},
   "source": [
    "## 5. AWQ ì–‘ìí™” ì‹¤í–‰\n",
    "\n",
    "> T4ì—ì„œ ì•½ 10~20ë¶„ ì†Œìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af099d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(f\"AWQ ì–‘ìí™” ì‹œì‘...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model.quantize(\n",
    "    tokenizer,\n",
    "    quant_config=AWQ_CONFIG,\n",
    "    calib_data=calib_data,\n",
    "    max_calib_seq_len=MAX_SEQUENCE_LENGTH,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nâœ… AWQ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e73b14",
   "metadata": {},
   "source": [
    "## 6. ì €ì¥ & ì œì¶œ íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print(f\"ëª¨ë¸ ì €ì¥ ì¤‘: {OUT_DIR}\")\n",
    "\n",
    "# AWQëŠ” save_quantized ì‚¬ìš©\n",
    "model.save_quantized(OUT_DIR, safetensors=True)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "total_size = sum(f.stat().st_size for f in Path(OUT_DIR).iterdir()) / 1024**3\n",
    "print(f\"\\nì´ í¬ê¸°: {total_size:.2f} GB\")\n",
    "\n",
    "# config.json í™•ì¸\n",
    "import json\n",
    "with open(f\"{OUT_DIR}/config.json\") as f:\n",
    "    cfg = json.load(f)\n",
    "print(f\"quantization_config: {cfg.get('quantization_config', 'ì—†ìŒ')}\")\n",
    "print(\"\\nâœ… ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf1d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(base_name=ZIP_NAME, format=\"zip\", root_dir=\".\", base_dir=OUT_DIR)\n",
    "zip_size = Path(f\"{ZIP_NAME}.zip\").stat().st_size / 1024**3\n",
    "print(f\"âœ… {ZIP_NAME}.zip ìƒì„± ì™„ë£Œ ({zip_size:.2f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9845117",
   "metadata": {},
   "source": [
    "## 7. Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779cfcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"ì €ì¥ëœ AWQ ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸...\")\n",
    "model_check = AutoAWQForCausalLM.from_quantized(\n",
    "    OUT_DIR,\n",
    "    trust_remote_code=True,\n",
    "    safetensors=True,\n",
    ")\n",
    "tok_check = AutoTokenizer.from_pretrained(OUT_DIR, trust_remote_code=True)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”\"}]\n",
    "inputs = tok_check.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model_check.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_check.generate(inputs, max_new_tokens=50, do_sample=False)\n",
    "\n",
    "response = tok_check.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
    "print(f\"ëª¨ë¸ ì‘ë‹µ: {response}\")\n",
    "print(\"\\nâœ… AWQ ëª¨ë¸ ì •ìƒ ì‘ë™!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf5ee10",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ê¸°ë¡\n",
    "\n",
    "| ì‹¤í—˜ | ë°©ì‹ | ìƒ˜í”Œ ìˆ˜ | group_size | LB Score | ë¹„ê³  |\n",
    "|------|------|---------|------------|----------|------|\n",
    "| AWQ ê¸°ë³¸ | W4A16 GEMM | 512 | 128 | - | |\n",
    "\n",
    "---\n",
    "## â­ï¸ ë¹„êµ ê²°ë¡ \n",
    "\n",
    "3ê°œ ì‹¤í—˜(GPTQ / FP8 / AWQ) LB ì ìˆ˜ ë¹„êµ í›„:\n",
    "- ê°€ì¥ ë†’ì€ ë°©ì‹ì„ ë©”ì¸ìœ¼ë¡œ ì±„íƒ\n",
    "- ì±„íƒëœ ë°©ì‹ ìœ„ì— `04_qlora_sft.ipynb`ë¡œ QLoRA SFT ì ìš©"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
