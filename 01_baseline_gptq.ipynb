{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dae9b152",
      "metadata": {
        "id": "dae9b152"
      },
      "source": [
        "# 01. GPTQ ë² ì´ìŠ¤ë¼ì¸ â€” EXAONE-4.0-1.2B\n",
        "\n",
        "**ëª©í‘œ:** ëŒ€íšŒ ë² ì´ìŠ¤ë¼ì¸(LB â‰ˆ 0.5)ì„ ì¬í˜„í•˜ê³ , ìº˜ë¦¬ë¸Œë ˆì´ì…˜ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•´ì„œ ì„±ëŠ¥ì„ ê°œì„ í•œë‹¤.\n",
        "\n",
        "**í™˜ê²½:** Google Colab Free (T4 16GB)  \n",
        "**í‰ê°€ ì„œë²„ í™˜ê²½:** L4 GPU, vLLM 0.14.1, Python 3.11\n",
        "\n",
        "---\n",
        "### ì „ëµ ìš”ì•½\n",
        "- ë² ì´ìŠ¤ë¼ì¸: GPTQ W4A16, ìº˜ë¦¬ë¸Œ 256ìƒ˜í”Œ, max_len 512 â†’ LB 0.5\n",
        "- ì‹¤í—˜ 1: ìº˜ë¦¬ë¸Œ ìƒ˜í”Œ ìˆ˜ â†‘ (512 / 1024) â†’ ì •í™•ë„ ê°œì„  ê¸°ëŒ€\n",
        "- ì‹¤í—˜ 2: max_sequence_length â†‘ (1024 / 2048) â†’ ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ ìº˜ë¦¬ë¸Œ\n",
        "- ì‹¤í—˜ 3: MANTA-1M ì™¸ ë°ì´í„°ì…‹ ì¡°í•©"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e6760fa",
      "metadata": {
        "id": "1e6760fa"
      },
      "source": [
        "## 0. í™˜ê²½ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45cf28e5",
      "metadata": {
        "id": "45cf28e5"
      },
      "outputs": [],
      "source": [
        "# GPU í™•ì¸\n",
        "import subprocess\n",
        "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "969468be",
      "metadata": {
        "id": "969468be"
      },
      "source": [
        "## 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "\n",
        "> âš ï¸ ì„¤ì¹˜ í›„ **ëŸ°íƒ€ì„ ì¬ì‹œì‘** í•„ìš”. ì•„ë˜ ì…€ ì‹¤í–‰ â†’ ëŸ°íƒ€ì„ ì¬ì‹œì‘ â†’ 2ë²ˆ ì…€ë¶€í„° ë‹¤ì‹œ ì‹¤í–‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9cee849",
      "metadata": {
        "id": "b9cee849"
      },
      "outputs": [],
      "source": [
        "# llmcompressor: ëŒ€íšŒ ë² ì´ìŠ¤ë¼ì¸ê³¼ ë™ì¼í•œ ì–‘ìí™” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "# compressed-tensors: í‰ê°€ ì„œë²„ì—ë„ ì„¤ì¹˜ëœ ë²„ì „(0.13.0)ê³¼ í˜¸í™˜\n",
        "!pip install -q \\\n",
        "    llmcompressor \\\n",
        "    \"compressed-tensors==0.13.0\" \\\n",
        "    \"transformers>=4.54.0\" \\\n",
        "    datasets \\\n",
        "    accelerate \\\n",
        "    sentencepiece\n",
        "\n",
        "print(\"\\nâœ… ì„¤ì¹˜ ì™„ë£Œ! ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ì„¸ìš” (ë©”ë‰´ â†’ ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95a83fd2",
      "metadata": {
        "id": "95a83fd2"
      },
      "source": [
        "## 2. ì„¤ì •ê°’ (ì—¬ê¸°ì„œ ì‹¤í—˜ íŒŒë¼ë¯¸í„° ë³€ê²½)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2beec642",
      "metadata": {
        "id": "2beec642"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ì‹¤í—˜ ì„¤ì • â€” ì—¬ê¸°ë§Œ ë°”ê¾¸ë©´ì„œ ì‹¤í—˜\n",
        "# ============================================================\n",
        "\n",
        "# ëª¨ë¸\n",
        "MODEL_ID = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"   # HuggingFaceì—ì„œ ì§ì ‘ ë¡œë“œ\n",
        "OUT_DIR  = \"./model\"                          # ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜\n",
        "\n",
        "# ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°\n",
        "DATASET_ID    = \"LGAI-EXAONE/MANTA-1M\"       # EXAONE ê³µì‹ ë°ì´í„°ì…‹ (ë² ì´ìŠ¤ë¼ì¸ê³¼ ë™ì¼)\n",
        "DATASET_SPLIT = \"train\"\n",
        "\n",
        "# â¬‡ï¸ ì‹¤í—˜ í¬ì¸íŠ¸ 1: ìƒ˜í”Œ ìˆ˜ ëŠ˜ë¦´ìˆ˜ë¡ ì •í™•ë„ UP, ì‹œê°„ UP\n",
        "# ë² ì´ìŠ¤ë¼ì¸=256 / ê¶Œì¥ì‹¤í—˜: 512 / ìµœëŒ€ê¶Œì¥(T4): 1024\n",
        "NUM_CALIBRATION_SAMPLES = 512\n",
        "\n",
        "# â¬‡ï¸ ì‹¤í—˜ í¬ì¸íŠ¸ 2: ì‹œí€€ìŠ¤ ê¸¸ì´ ëŠ˜ë¦´ìˆ˜ë¡ ìº˜ë¦¬ë¸Œ í’ˆì§ˆ UP, ë©”ëª¨ë¦¬ UP\n",
        "# ë² ì´ìŠ¤ë¼ì¸=512 / ê¶Œì¥ì‹¤í—˜: 1024 / T4 í•œê³„: ~1024\n",
        "MAX_SEQUENCE_LENGTH = 1024\n",
        "\n",
        "# ì–‘ìí™” ì„¤ì • (ë² ì´ìŠ¤ë¼ì¸ê³¼ ë™ì¼ â€” W4A16: ê°€ì¤‘ì¹˜ 4bit, í™œì„±í™” 16bit)\n",
        "SCHEME  = \"W4A16\"\n",
        "TARGETS = [\"Linear\"]\n",
        "IGNORE  = [\"embed_tokens\", \"lm_head\"]  # ì„ë² ë”©/ì¶œë ¥ ë ˆì´ì–´ëŠ” ì–‘ìí™” ì œì™¸\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ëª…\n",
        "ZIP_NAME = \"submit_gptq_w4a16\"\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(f\"ëª¨ë¸        : {MODEL_ID}\")\n",
        "print(f\"ìº˜ë¦¬ë¸Œ ìƒ˜í”Œ : {NUM_CALIBRATION_SAMPLES}\")\n",
        "print(f\"ì‹œí€€ìŠ¤ ê¸¸ì´ : {MAX_SEQUENCE_LENGTH}\")\n",
        "print(f\"ì–‘ìí™” ë°©ì‹ : {SCHEME}\")\n",
        "print(f\"ì €ì¥ ê²½ë¡œ   : {OUT_DIR}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9c90869",
      "metadata": {
        "id": "a9c90869"
      },
      "source": [
        "## 3. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "\n",
        "> T4 16GBì—ì„œ 1.2B bfloat16 ëª¨ë¸ì€ ì•½ 2.5GB â†’ ì—¬ìœ  ìˆìŒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7cb9b2d",
      "metadata": {
        "id": "d7cb9b2d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(\"[1/4] í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "print(f\"      vocab size: {tokenizer.vocab_size:,}\")\n",
        "\n",
        "print(\"[2/4] ëª¨ë¸ ë¡œë“œ ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ, ì•½ 2.5GB)\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    # device_map ë¯¸ì§€ì • â†’ llmcompressorê°€ ì§ì ‘ GPU ë°°ì¹˜ ê´€ë¦¬\n",
        ")\n",
        "\n",
        "# ëª¨ë¸ êµ¬ì¡° ê°„ëµ í™•ì¸\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"      ì´ íŒŒë¼ë¯¸í„°: {total_params/1e9:.2f}B\")\n",
        "print(f\"      ë ˆì´ì–´ ìˆ˜  : {model.config.num_hidden_layers}\")\n",
        "print(f\"      Attention  : {model.config.num_attention_heads} heads / {model.config.num_key_value_heads} KV heads (GQA)\")\n",
        "print(\"\\nâœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd136939",
      "metadata": {
        "id": "dd136939"
      },
      "source": [
        "## 4. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ì¤€ë¹„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c4050d0",
      "metadata": {
        "id": "9c4050d0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(f\"[3/4] ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ë¡œë“œ ì¤‘: {DATASET_ID} ({NUM_CALIBRATION_SAMPLES}ìƒ˜í”Œ)\")\n",
        "\n",
        "ds = load_dataset(\n",
        "    DATASET_ID,\n",
        "    split=f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\",\n",
        ")\n",
        "\n",
        "def preprocess(example):\n",
        "    \"\"\"ëŒ€í™” í˜•ì‹ì„ EXAONE chat templateì— ë§ê²Œ ë³€í™˜\"\"\"\n",
        "    return {\n",
        "        \"text\": tokenizer.apply_chat_template(\n",
        "            example[\"conversations\"],\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=False,\n",
        "        )\n",
        "    }\n",
        "\n",
        "ds = ds.map(preprocess)\n",
        "\n",
        "# ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°\n",
        "print(\"\\n--- ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²« ë²ˆì§¸ í•­ëª© ì• 200ì) ---\")\n",
        "print(ds[0][\"text\"][:200])\n",
        "print(\"...\")\n",
        "print(f\"\\nâœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(ds)}ê°œ ìƒ˜í”Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "763f7b12",
      "metadata": {
        "id": "763f7b12"
      },
      "source": [
        "## 5. GPTQ ì–‘ìí™” ì‹¤í–‰\n",
        "\n",
        "> T4ì—ì„œ 512ìƒ˜í”Œ ê¸°ì¤€ ì•½ 15~25ë¶„ ì†Œìš”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74041e05",
      "metadata": {
        "id": "74041e05"
      },
      "outputs": [],
      "source": [
        "from llmcompressor import oneshot\n",
        "from llmcompressor.modifiers.quantization import GPTQModifier\n",
        "import time\n",
        "\n",
        "print(f\"[4/4] GPTQ ì–‘ìí™” ì‹œì‘\")\n",
        "print(f\"      Scheme : {SCHEME}\")\n",
        "print(f\"      Samples: {NUM_CALIBRATION_SAMPLES}\")\n",
        "print(f\"      MaxLen : {MAX_SEQUENCE_LENGTH}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "recipe = [\n",
        "    GPTQModifier(\n",
        "        scheme=SCHEME,\n",
        "        targets=TARGETS,\n",
        "        ignore=IGNORE,\n",
        "    )\n",
        "]\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "oneshot(\n",
        "    model=model,\n",
        "    dataset=ds,\n",
        "    recipe=recipe,\n",
        "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
        ")\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\nâœ… GPTQ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55793b71",
      "metadata": {
        "id": "55793b71"
      },
      "source": [
        "## 6. ëª¨ë¸ ì €ì¥ & ì œì¶œ íŒŒì¼ ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a0e3c4",
      "metadata": {
        "id": "26a0e3c4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# ì €ì¥\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "print(f\"ëª¨ë¸ ì €ì¥ ì¤‘: {OUT_DIR}\")\n",
        "\n",
        "model.save_pretrained(OUT_DIR, save_compressed=True)\n",
        "tokenizer.save_pretrained(OUT_DIR)\n",
        "\n",
        "# ì €ì¥ëœ íŒŒì¼ í™•ì¸\n",
        "print(\"\\nì €ì¥ëœ íŒŒì¼:\")\n",
        "for f in sorted(Path(OUT_DIR).iterdir()):\n",
        "    size_mb = f.stat().st_size / 1024**2\n",
        "    print(f\"  {f.name:40s} {size_mb:8.1f} MB\")\n",
        "\n",
        "total_size = sum(f.stat().st_size for f in Path(OUT_DIR).iterdir()) / 1024**3\n",
        "print(f\"\\nì´ í¬ê¸°: {total_size:.2f} GB (ì œì¶œ í•œë„: ì••ì¶• í›„ 10GB / ì••ì¶• ì „ 32GB)\")\n",
        "\n",
        "print(\"\\nâœ… ì €ì¥ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "947a4398",
      "metadata": {
        "id": "947a4398"
      },
      "outputs": [],
      "source": [
        "# ì œì¶œìš© zip ìƒì„±\n",
        "print(f\"ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘: {ZIP_NAME}.zip\")\n",
        "\n",
        "shutil.make_archive(\n",
        "    base_name=ZIP_NAME,\n",
        "    format=\"zip\",\n",
        "    root_dir=\".\",\n",
        "    base_dir=OUT_DIR,\n",
        ")\n",
        "\n",
        "zip_size = Path(f\"{ZIP_NAME}.zip\").stat().st_size / 1024**3\n",
        "print(f\"\\nâœ… {ZIP_NAME}.zip ìƒì„± ì™„ë£Œ\")\n",
        "print(f\"   ì••ì¶• íŒŒì¼ í¬ê¸°: {zip_size:.2f} GB\")\n",
        "print(\"   â†’ Dacon ì œì¶œ í˜ì´ì§€ì— ì´ zip íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0387495f",
      "metadata": {
        "id": "0387495f"
      },
      "source": [
        "## 7. ë¹ ë¥¸ sanity check (ì–‘ìí™” ê²°ê³¼ í™•ì¸)\n",
        "\n",
        "ì œì¶œ ì „ì— ì–‘ìí™”ëœ ëª¨ë¸ì´ ì •ìƒ ì¶”ë¡ ë˜ëŠ”ì§€ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0116d0ef",
      "metadata": {
        "id": "0116d0ef"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "print(\"ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "model_check = AutoModelForCausalLM.from_pretrained(\n",
        "    OUT_DIR,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tok_check = AutoTokenizer.from_pretrained(OUT_DIR, trust_remote_code=True)\n",
        "\n",
        "# ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
        "prompt = \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "inputs = tok_check.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model_check.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model_check.generate(inputs, max_new_tokens=50, do_sample=False)\n",
        "\n",
        "response = tok_check.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "print(f\"\\ní…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: {prompt}\")\n",
        "print(f\"ëª¨ë¸ ì‘ë‹µ: {response}\")\n",
        "print(\"\\nâœ… Sanity check ì™„ë£Œ! ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c4adeff",
      "metadata": {
        "id": "7c4adeff"
      },
      "source": [
        "---\n",
        "## ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ê¸°ë¡\n",
        "\n",
        "ì•„ë˜ í‘œì— ì‹¤í—˜í•  ë•Œë§ˆë‹¤ ê²°ê³¼ë¥¼ ê¸°ë¡í•´ë‘ì„¸ìš”:\n",
        "\n",
        "| ì‹¤í—˜ | SCHEME | ìƒ˜í”Œ ìˆ˜ | MaxLen | ë°ì´í„°ì…‹ | LB Score | ë¹„ê³  |\n",
        "|------|--------|---------|--------|----------|----------|------|\n",
        "| ë² ì´ìŠ¤ë¼ì¸ | W4A16 | 256 | 512 | MANTA-1M | 0.5 | ëŒ€íšŒ ì œê³µ ë² ì´ìŠ¤ë¼ì¸ |\n",
        "| ì‹¤í—˜1 | W4A16 | 512 | 1024 | MANTA-1M | - | ì´ ë…¸íŠ¸ë¶ ê¸°ë³¸ ì„¤ì • |\n",
        "| ì‹¤í—˜2 | W4A16 | 1024 | 1024 | MANTA-1M | - | ìƒ˜í”Œ ìˆ˜ 2ë°° |\n",
        "\n",
        "---\n",
        "## â­ï¸ ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "- GPTQ ê²°ê³¼ë¥¼ ë³´ê³  ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ â†’ `02_fp8_quantization.ipynb` ë¡œ FP8 ì‹œë„ (L4ì—ì„œ í•˜ë“œì›¨ì–´ ê°€ì† ê¸°ëŒ€)\n",
        "- ì ìˆ˜ê°€ ì˜ ë‚˜ì˜¤ë©´ â†’ `04_qlora_sft.ipynb` ë¡œ ì„±ëŠ¥ ì¶”ê°€ íšŒë³µ"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}