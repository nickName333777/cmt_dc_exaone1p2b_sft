{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5d2de6",
   "metadata": {},
   "source": [
    "# 04. QLoRA SFT â€” ì–‘ìí™” í›„ ì„±ëŠ¥ íšŒë³µ\n",
    "\n",
    "**ì „ëµ:** ì–‘ìí™”ë¡œ ë–¨ì–´ì§„ ì •í™•ë„ë¥¼ QLoRA fine-tuningìœ¼ë¡œ íšŒë³µ.  \n",
    "ì´ê²Œ ìƒìœ„ê¶Œì„ ì§„ì§œë¡œ ë…¸ë¦´ ìˆ˜ ìˆëŠ” ì°¨ë³„í™” í¬ì¸íŠ¸.\n",
    "\n",
    "**íŒŒì´í”„ë¼ì¸:**  \n",
    "```\n",
    "EXAONE-4.0-1.2B (ì›ë³¸ BF16)\n",
    "    â†“ [ì´ ë…¸íŠ¸ë¶]\n",
    "QLoRA SFT (4-bit bitsandbytes ë¡œë“œ + LoRA í•™ìŠµ)\n",
    "    â†“\n",
    "LoRA ê°€ì¤‘ì¹˜ merge â†’ BF16 í’€ëª¨ë¸\n",
    "    â†“ [01 ë˜ëŠ” 02 ë…¸íŠ¸ë¶]\n",
    "GPTQ/FP8 ì–‘ìí™” â†’ ì œì¶œ\n",
    "```\n",
    "\n",
    "**í™˜ê²½:** Google Colab Free (T4 16GB)  \n",
    "âš ï¸ T4 16GBì—ì„œëŠ” QLoRA rankë¥¼ ì‘ê²Œ ìœ ì§€í•´ì•¼ ë©”ëª¨ë¦¬ ì•ˆì „"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf52b41",
   "metadata": {},
   "source": [
    "## 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "\n",
    "> âš ï¸ ì„¤ì¹˜ í›„ **ëŸ°íƒ€ì„ ì¬ì‹œì‘** í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbf861",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "    \"transformers>=4.54.0\" \\\n",
    "    peft \\\n",
    "    trl \\\n",
    "    bitsandbytes \\\n",
    "    datasets \\\n",
    "    accelerate \\\n",
    "    sentencepiece \\\n",
    "    llmcompressor \\\n",
    "    \"compressed-tensors==0.13.0\"\n",
    "\n",
    "print(\"\\nâœ… ì„¤ì¹˜ ì™„ë£Œ! ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8dbd06",
   "metadata": {},
   "source": [
    "## 2. í™˜ê²½ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"GPU : {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "import transformers, peft, trl\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"peft        : {peft.__version__}\")\n",
    "print(f\"trl         : {trl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18209a51",
   "metadata": {},
   "source": [
    "## 3. ì„¤ì •ê°’\n",
    "\n",
    "> T4 16GB ê¸°ì¤€ ì•ˆì „í•œ ì„¤ì •. ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ `LORA_RANK` ë˜ëŠ” `BATCH_SIZE` ì¤„ì¼ ê²ƒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ccf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QLoRA SFT ì„¤ì •\n",
    "# ============================================================\n",
    "\n",
    "MODEL_ID     = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
    "SFT_OUT_DIR  = \"./model_sft_lora\"      # LoRA adapter ì €ì¥\n",
    "MERGED_DIR   = \"./model_sft_merged\"    # LoRA mergeëœ í’€ëª¨ë¸\n",
    "QUANT_OUT_DIR = \"./model\"              # ìµœì¢… ì–‘ìí™” ê²°ê³¼ (ì œì¶œìš©)\n",
    "\n",
    "# ë°ì´í„°ì…‹: MANTA-1M (í‰ê°€ íƒœìŠ¤í¬ì™€ aligned)\n",
    "DATASET_ID    = \"LGAI-EXAONE/MANTA-1M\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "NUM_TRAIN_SAMPLES = 2000   # T4ì—ì„œ í˜„ì‹¤ì ì¸ í•™ìŠµëŸ‰\n",
    "MAX_SEQ_LENGTH    = 512    # T4 16GB ë©”ëª¨ë¦¬ ì•ˆì „ ë²”ìœ„\n",
    "\n",
    "# LoRA ì„¤ì • (T4 16GB ìµœì í™”)\n",
    "LORA_RANK        = 8       # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 4ë¡œ ì¤„ì´ê¸°\n",
    "LORA_ALPHA       = 16      # ë³´í†µ rank * 2\n",
    "LORA_DROPOUT     = 0.05\n",
    "LORA_TARGET      = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"]  # ëª¨ë“  Linear íƒ€ê²Ÿ\n",
    "\n",
    "# í•™ìŠµ ì„¤ì •\n",
    "BATCH_SIZE       = 1       # T4ì—ì„œ ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°\n",
    "GRAD_ACCUM       = 8       # íš¨ê³¼ì  ë°°ì¹˜ = 1 * 8 = 8\n",
    "LEARNING_RATE    = 2e-4\n",
    "NUM_EPOCHS       = 1\n",
    "WARMUP_RATIO     = 0.03\n",
    "\n",
    "ZIP_NAME = \"submit_sft_gptq\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"ëª¨ë¸         : {MODEL_ID}\")\n",
    "print(f\"í•™ìŠµ ìƒ˜í”Œ    : {NUM_TRAIN_SAMPLES}\")\n",
    "print(f\"LoRA rank    : {LORA_RANK}\")\n",
    "print(f\"í•™ìŠµë¥        : {LEARNING_RATE}\")\n",
    "print(f\"Effective BS : {BATCH_SIZE * GRAD_ACCUM}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d3469e",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ ë¡œë“œ (4-bit bitsandbytesë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc293287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# 4-bit bitsandbytes ì„¤ì • (í•™ìŠµ ì¤‘ ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,   # ì´ì¤‘ ì–‘ìí™”ë¡œ ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "    bnb_4bit_quant_type=\"nf4\",        # NF4: QLoRA ë…¼ë¬¸ ê¶Œì¥\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"ëª¨ë¸ ë¡œë“œ ì¤‘ (4-bit bitsandbytes)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# gradient checkpointing: ë©”ëª¨ë¦¬ ì ˆì•½ (ì†ë„ëŠ” ì•½ê°„ ëŠë ¤ì§)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "print(f\"\\në¡œë“œ í›„ GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe612944",
   "metadata": {},
   "source": [
    "## 5. LoRA ì–´ëŒ‘í„° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# k-bit í•™ìŠµ ì¤€ë¹„ (bitsandbytes 4-bit ëª¨ë¸ìš©)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "print(f\"í•™ìŠµ íŒŒë¼ë¯¸í„°: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "print(\"âœ… LoRA ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7a466",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(f\"í•™ìŠµ ë°ì´í„° ë¡œë“œ: {NUM_TRAIN_SAMPLES}ìƒ˜í”Œ\")\n",
    "ds = load_dataset(DATASET_ID, split=f\"{DATASET_SPLIT}[:{NUM_TRAIN_SAMPLES}]\")\n",
    "\n",
    "def preprocess(example):\n",
    "    \"\"\"SFTë¥¼ ìœ„í•œ ì „ì²˜ë¦¬: full conversationì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ\"\"\"\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"conversations\"],\n",
    "            add_generation_prompt=False,  # SFTëŠ” False (ì‘ë‹µê¹Œì§€ í¬í•¨)\n",
    "            tokenize=False,\n",
    "        )\n",
    "    }\n",
    "\n",
    "ds = ds.map(preprocess, remove_columns=ds.column_names)\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ì¤€ë¹„: {len(ds)}ê°œ\")\n",
    "print(f\"ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°:\\n{ds[0]['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49784b79",
   "metadata": {},
   "source": [
    "## 7. SFT í•™ìŠµ\n",
    "\n",
    "> T4ì—ì„œ 2000ìƒ˜í”Œ 1 epoch â‰ˆ 20~40ë¶„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=SFT_OUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",       # ë©”ëª¨ë¦¬ íš¨ìœ¨ optimizer\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=\"none\",               # wandb ë“± ì™¸ë¶€ ë¡œê¹… ë¹„í™œì„±í™”\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds,\n",
    "    args=training_args,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"SFT í•™ìŠµ ì‹œì‘!\")\n",
    "trainer.train()\n",
    "print(\"\\nâœ… SFT í•™ìŠµ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b56ce9",
   "metadata": {},
   "source": [
    "## 8. LoRA Merge â†’ BF16 í’€ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"LoRA adapter ì €ì¥ ì¤‘...\")\n",
    "model.save_pretrained(SFT_OUT_DIR)\n",
    "tokenizer.save_pretrained(SFT_OUT_DIR)\n",
    "\n",
    "print(\"\\nLoRA mergeë¥¼ ìœ„í•´ ì›ë³¸ BF16 ëª¨ë¸ ì¬ë¡œë“œ ì¤‘...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"LoRA merge ì¤‘...\")\n",
    "peft_model = PeftModel.from_pretrained(base_model, SFT_OUT_DIR)\n",
    "merged_model = peft_model.merge_and_unload()  # LoRAë¥¼ baseì— í†µí•©\n",
    "\n",
    "print(f\"mergeëœ ëª¨ë¸ ì €ì¥ ì¤‘: {MERGED_DIR}\")\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "merged_model.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
    "tokenizer.save_pretrained(MERGED_DIR)\n",
    "\n",
    "print(\"\\nâœ… LoRA merge ì™„ë£Œ!\")\n",
    "print(f\"   â†’ ë‹¤ìŒ ë‹¨ê³„: {MERGED_DIR} ë¥¼ 01 ë˜ëŠ” 02 ë…¸íŠ¸ë¶ì—ì„œ ì–‘ìí™”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8951ce56",
   "metadata": {},
   "source": [
    "## 9. SFT Merge ëª¨ë¸ ì–‘ìí™” (GPTQ W4A16)\n",
    "\n",
    "mergeëœ BF16 ëª¨ë¸ì„ ë°”ë¡œ ì´ì–´ì„œ ì–‘ìí™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1906dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "del merged_model, peft_model, base_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {torch.cuda.memory_allocated()/1024**3:.2f} GB ì‚¬ìš© ì¤‘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "import time\n",
    "\n",
    "# SFT mergeëœ ëª¨ë¸ë¡œ ì–‘ìí™”\n",
    "QUANT_SOURCE = MERGED_DIR   # SFT merge ê²°ê³¼\n",
    "QUANT_CALIB_SAMPLES = 512\n",
    "QUANT_MAX_LEN = 1024\n",
    "\n",
    "print(f\"SFT merge ëª¨ë¸ ë¡œë“œ: {QUANT_SOURCE}\")\n",
    "tokenizer_q = AutoTokenizer.from_pretrained(QUANT_SOURCE, trust_remote_code=True)\n",
    "model_q = AutoModelForCausalLM.from_pretrained(\n",
    "    QUANT_SOURCE,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°\n",
    "ds_q = load_dataset(DATASET_ID, split=f\"{DATASET_SPLIT}[:{QUANT_CALIB_SAMPLES}]\")\n",
    "ds_q = ds_q.map(lambda ex: {\"text\": tokenizer_q.apply_chat_template(\n",
    "    ex[\"conversations\"], add_generation_prompt=True, tokenize=False)})\n",
    "\n",
    "# GPTQ ì–‘ìí™”\n",
    "print(\"GPTQ ì–‘ìí™” ì‹œì‘...\")\n",
    "start = time.time()\n",
    "recipe = [GPTQModifier(scheme=\"W4A16\", targets=[\"Linear\"], ignore=[\"embed_tokens\", \"lm_head\"])]\n",
    "oneshot(model=model_q, dataset=ds_q, recipe=recipe,\n",
    "        max_seq_length=QUANT_MAX_LEN, num_calibration_samples=QUANT_CALIB_SAMPLES)\n",
    "\n",
    "print(f\"ì–‘ìí™” ì™„ë£Œ ({(time.time()-start)/60:.1f}ë¶„)\")\n",
    "\n",
    "# ì €ì¥\n",
    "import os, shutil\n",
    "from pathlib import Path\n",
    "os.makedirs(QUANT_OUT_DIR, exist_ok=True)\n",
    "model_q.save_pretrained(QUANT_OUT_DIR, save_compressed=True)\n",
    "tokenizer_q.save_pretrained(QUANT_OUT_DIR)\n",
    "\n",
    "# ì œì¶œ zip\n",
    "shutil.make_archive(base_name=ZIP_NAME, format=\"zip\", root_dir=\".\", base_dir=QUANT_OUT_DIR)\n",
    "zip_size = Path(f\"{ZIP_NAME}.zip\").stat().st_size / 1024**3\n",
    "print(f\"\\nâœ… ì™„ë£Œ! {ZIP_NAME}.zip ({zip_size:.2f} GB)\")\n",
    "print(\"   â†’ Dacon ì œì¶œ í˜ì´ì§€ì— ì—…ë¡œë“œí•˜ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82031f4",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š SFT ì‹¤í—˜ ê¸°ë¡\n",
    "\n",
    "| ì‹¤í—˜ | í•™ìŠµ ìƒ˜í”Œ | Rank | Epoch | ì´í›„ ì–‘ìí™” | LB Score | ë¹„ê³  |\n",
    "|------|----------|------|-------|------------|----------|------|\n",
    "| SFT-1 | 2000 | 8 | 1 | GPTQ W4A16 | - | ê¸°ë³¸ ì„¤ì • |\n",
    "\n",
    "---\n",
    "## âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ëŒ€ì²˜ë²•\n",
    "\n",
    "1. `LORA_RANK` ë¥¼ 8 â†’ 4 ë¡œ ì¤„ì´ê¸°\n",
    "2. `NUM_TRAIN_SAMPLES` ë¥¼ 2000 â†’ 1000 ìœ¼ë¡œ ì¤„ì´ê¸°\n",
    "3. `MAX_SEQ_LENGTH` ë¥¼ 512 â†’ 256 ìœ¼ë¡œ ì¤„ì´ê¸°\n",
    "4. Colab Proë¡œ ì—…ê·¸ë ˆì´ë“œ (A100 ì‚¬ìš© ê°€ëŠ¥)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
